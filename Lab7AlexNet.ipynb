{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680b2e68-1c3d-42ae-8f42-cf94a82f64e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:14:10.064840Z",
     "iopub.status.busy": "2023-09-28T20:14:10.064209Z",
     "iopub.status.idle": "2023-09-28T20:14:14.134126Z",
     "shell.execute_reply": "2023-09-28T20:14:14.133371Z",
     "shell.execute_reply.started": "2023-09-28T20:14:10.064816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformaciones: Convertir imágenes a tensores y normalizar\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización para 3 canales (RGB)\n",
    "])\n",
    "\n",
    "# Cargar CIFAR100 dataset usando torchvision\n",
    "train_dataset = CIFAR100(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR100(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Cargar los datos en batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd1f13f-d6a3-4478-b59c-4510e95dda18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:14:14.136242Z",
     "iopub.status.busy": "2023-09-28T20:14:14.135494Z",
     "iopub.status.idle": "2023-09-28T20:14:14.286172Z",
     "shell.execute_reply": "2023-09-28T20:14:14.285517Z",
     "shell.execute_reply.started": "2023-09-28T20:14:14.136219Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = AlexNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3834a21-32e7-4980-b887-c24673879a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:14:14.287638Z",
     "iopub.status.busy": "2023-09-28T20:14:14.287150Z",
     "iopub.status.idle": "2023-09-28T20:14:18.648108Z",
     "shell.execute_reply": "2023-09-28T20:14:18.647489Z",
     "shell.execute_reply.started": "2023-09-28T20:14:14.287635Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/alexnet_experiment_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4994eee6-bac3-4b62-9de8-400a394bbe75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:36:24.320302Z",
     "iopub.status.busy": "2023-09-28T20:36:24.319761Z",
     "iopub.status.idle": "2023-09-28T20:38:37.380419Z",
     "shell.execute_reply": "2023-09-28T20:38:37.379711Z",
     "shell.execute_reply.started": "2023-09-28T20:36:24.320280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Accuracy: 0.3251\n",
      "Epoch 2/50, Accuracy: 0.3313\n",
      "Epoch 3/50, Accuracy: 0.3327\n",
      "Epoch 4/50, Accuracy: 0.3302\n",
      "Epoch 5/50, Accuracy: 0.3394\n",
      "Epoch 6/50, Accuracy: 0.3450\n",
      "Epoch 7/50, Accuracy: 0.3372\n",
      "Epoch 8/50, Accuracy: 0.3460\n",
      "Epoch 9/50, Accuracy: 0.3469\n",
      "Epoch 10/50, Accuracy: 0.3478\n",
      "Epoch 11/50, Accuracy: 0.3469\n",
      "Epoch 12/50, Accuracy: 0.3446\n",
      "Epoch 13/50, Accuracy: 0.3498\n",
      "Epoch 14/50, Accuracy: 0.3475\n",
      "Epoch 15/50, Accuracy: 0.3578\n",
      "Epoch 16/50, Accuracy: 0.3494\n",
      "Epoch 17/50, Accuracy: 0.3457\n",
      "Epoch 18/50, Accuracy: 0.3441\n",
      "Epoch 19/50, Accuracy: 0.3485\n",
      "Epoch 20/50, Accuracy: 0.3426\n",
      "Epoch 21/50, Accuracy: 0.3466\n",
      "Epoch 22/50, Accuracy: 0.3472\n",
      "Epoch 23/50, Accuracy: 0.3404\n",
      "Epoch 24/50, Accuracy: 0.3516\n",
      "Epoch 25/50, Accuracy: 0.3465\n",
      "Epoch 26/50, Accuracy: 0.3494\n",
      "Epoch 27/50, Accuracy: 0.3457\n",
      "Epoch 28/50, Accuracy: 0.3484\n",
      "Epoch 29/50, Accuracy: 0.3409\n",
      "Epoch 30/50, Accuracy: 0.3364\n",
      "Epoch 31/50, Accuracy: 0.3462\n",
      "Epoch 32/50, Accuracy: 0.3377\n",
      "Epoch 33/50, Accuracy: 0.3481\n",
      "Epoch 34/50, Accuracy: 0.3399\n",
      "Epoch 35/50, Accuracy: 0.3451\n",
      "Epoch 36/50, Accuracy: 0.3357\n",
      "Epoch 37/50, Accuracy: 0.3323\n",
      "Epoch 38/50, Accuracy: 0.3415\n",
      "Epoch 39/50, Accuracy: 0.3400\n",
      "Epoch 40/50, Accuracy: 0.3350\n",
      "Epoch 41/50, Accuracy: 0.3391\n",
      "Epoch 42/50, Accuracy: 0.3342\n",
      "Epoch 43/50, Accuracy: 0.3311\n",
      "Epoch 44/50, Accuracy: 0.3417\n",
      "Epoch 45/50, Accuracy: 0.3401\n",
      "Epoch 46/50, Accuracy: 0.3401\n",
      "Epoch 47/50, Accuracy: 0.3431\n",
      "Epoch 48/50, Accuracy: 0.3372\n",
      "Epoch 49/50, Accuracy: 0.3336\n",
      "Epoch 50/50, Accuracy: 0.3384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Añadir el modelo a TensorBoard\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "writer.add_graph(model, images.to(device))\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * len(train_loader) + i)\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(test_dataset)\n",
    "    writer.add_scalar('accuracy', accuracy, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa7bf10",
   "metadata": {},
   "source": [
    "# AlexNet Learning\n",
    "![AlexNet Learning](./AlexNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dbb4a1",
   "metadata": {},
   "source": [
    "### Respuestas a las Preguntas:  \n",
    "#### a. Diferencia principal entre ambas arquitecturas:\n",
    "LeNet-5 fue una de las primeras arquitecturas de redes neuronales convolucionales, diseñada principalmente para reconocer dígitos. Su diseño es más sencillo y tiene menos capas y parámetros. AlexNet, por otro lado, es mucho más profunda y fue diseñada para tratar con datasets de imágenes más complejos y de mayor resolución, como ImageNet. Utiliza más capas convolucionales, capas completamente conectadas más grandes y técnicas modernas como la activación ReLU y el dropout.\n",
    "\n",
    "#### b. ¿Podría usarse LeNet-5 para un problema como el que resolvió usando AlexNet? ¿Y viceversa?  \n",
    "Sí, ambas redes pueden ser usadas para ambos datasets, pero la eficacia variará. Si usamos LeNet-5 en datasets más complicados como CIFAR10 o ImageNet, podría no tener el poder representacional suficiente para lograr un alto rendimiento debido a su simplicidad. AlexNet, aunque es más pesado, podría usarse para MNIST, pero es probable que sea excesivo y no tan eficiente en términos de recursos computacionales.\n",
    "\n",
    "#### c. Qué le pareció más interesante de cada arquitectura:  \n",
    "\n",
    "**LeNet-5:** Es asombroso cómo esta arquitectura temprana, con su diseño sencillo, sentó las bases para las CNNs futuras. Su eficacia en la clasificación de dígitos a mano fue revolucionaria en su momento.  \n",
    "**AlexNet:** Lo que es destacable de AlexNet es cómo utilizó técnicas modernas y una arquitectura más profunda para lograr un rendimiento nunca antes visto en ImageNet, superando a otros enfoques tradicionales de procesamiento de imágenes. También es notable cómo popularizó las GPUs para entrenar redes neuronales, dado que fue entrenada usando GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
