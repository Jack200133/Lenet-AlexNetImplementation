{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680b2e68-1c3d-42ae-8f42-cf94a82f64e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:14:10.064840Z",
     "iopub.status.busy": "2023-09-28T20:14:10.064209Z",
     "iopub.status.idle": "2023-09-28T20:14:14.134126Z",
     "shell.execute_reply": "2023-09-28T20:14:14.133371Z",
     "shell.execute_reply.started": "2023-09-28T20:14:10.064816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Transformaciones: Convertir imágenes a tensores y normalizar\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalización para 3 canales (RGB)\n",
    "])\n",
    "\n",
    "# Cargar CIFAR100 dataset usando torchvision\n",
    "train_dataset = CIFAR100(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR100(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Cargar los datos en batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd1f13f-d6a3-4478-b59c-4510e95dda18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:14:14.136242Z",
     "iopub.status.busy": "2023-09-28T20:14:14.135494Z",
     "iopub.status.idle": "2023-09-28T20:14:14.286172Z",
     "shell.execute_reply": "2023-09-28T20:14:14.285517Z",
     "shell.execute_reply.started": "2023-09-28T20:14:14.136219Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 2 * 2, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = AlexNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3834a21-32e7-4980-b887-c24673879a6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-28T20:14:14.287638Z",
     "iopub.status.busy": "2023-09-28T20:14:14.287150Z",
     "iopub.status.idle": "2023-09-28T20:14:18.648108Z",
     "shell.execute_reply": "2023-09-28T20:14:18.647489Z",
     "shell.execute_reply.started": "2023-09-28T20:14:14.287635Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/alexnet_experiment_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c60a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Accuracy: 0.0772, Precision: 0.0000, Recall: 0.0000, F1-score: 0.0000\n",
      "Epoch 2/10, Accuracy: 0.1322, Precision: 1.0000, Recall: 0.2857, F1-score: 0.4444\n",
      "Epoch 3/10, Accuracy: 0.1803, Precision: 0.7500, Recall: 1.0000, F1-score: 0.8571\n",
      "Epoch 4/10, Accuracy: 0.2183, Precision: 0.7714, Recall: 0.9643, F1-score: 0.8571\n",
      "Epoch 5/10, Accuracy: 0.2446, Precision: 0.8056, Recall: 0.9667, F1-score: 0.8788\n",
      "Epoch 6/10, Accuracy: 0.2680, Precision: 0.9677, Recall: 1.0000, F1-score: 0.9836\n",
      "Epoch 7/10, Accuracy: 0.2805, Precision: 0.9143, Recall: 0.9412, F1-score: 0.9275\n",
      "Epoch 8/10, Accuracy: 0.3020, Precision: 0.9565, Recall: 0.9362, F1-score: 0.9462\n",
      "Epoch 9/10, Accuracy: 0.3053, Precision: 0.9250, Recall: 1.0000, F1-score: 0.9610\n",
      "Epoch 10/10, Accuracy: 0.3140, Precision: 0.9074, Recall: 1.0000, F1-score: 0.9515\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Añadir el modelo a TensorBoard\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "writer.add_graph(model, images.to(device))\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            writer.add_scalar('training loss', running_loss / 100, epoch * len(train_loader) + i)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total_correct += (predicted == target).sum().item()\n",
    "\n",
    "            # Calculo para clasificación binaria\n",
    "            TP += ((predicted == 1) & (target == 1)).sum().item()\n",
    "            TN += ((predicted == 0) & (target == 0)).sum().item()\n",
    "            FP += ((predicted == 1) & (target == 0)).sum().item()\n",
    "            FN += ((predicted == 0) & (target == 1)).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(test_dataset)\n",
    "    precision = TP / (TP + FP) if TP + FP != 0 else 0\n",
    "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if precision + recall != 0 else 0\n",
    "\n",
    "    writer.add_scalar('accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('precision', precision, epoch)\n",
    "    writer.add_scalar('recall', recall, epoch)\n",
    "    writer.add_scalar('f1', f1, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54485571",
   "metadata": {},
   "source": [
    "# AlexNet Learning\n",
    "![AlexNet Learning](./AlexNet.png)\n",
    "\n",
    "### Respuestas a las Preguntas:  \n",
    "#### a. Diferencia principal entre ambas arquitecturas:\n",
    "LeNet-5 fue una de las primeras arquitecturas de redes neuronales convolucionales, diseñada principalmente para reconocer dígitos. Su diseño es más sencillo y tiene menos capas y parámetros. AlexNet, por otro lado, es mucho más profunda y fue diseñada para tratar con datasets de imágenes más complejos y de mayor resolución, como ImageNet. Utiliza más capas convolucionales, capas completamente conectadas más grandes y técnicas modernas como la activación ReLU y el dropout.\n",
    "\n",
    "#### b. ¿Podría usarse LeNet-5 para un problema como el que resolvió usando AlexNet? ¿Y viceversa?  \n",
    "Sí, ambas redes pueden ser usadas para ambos datasets, pero la eficacia variará. Si usamos LeNet-5 en datasets más complicados como CIFAR10 o ImageNet, podría no tener el poder representacional suficiente para lograr un alto rendimiento debido a su simplicidad. AlexNet, aunque es más pesado, podría usarse para MNIST, pero es probable que sea excesivo y no tan eficiente en términos de recursos computacionales.\n",
    "\n",
    "#### c. Qué le pareció más interesante de cada arquitectura:  \n",
    "\n",
    "**LeNet-5:** Es asombroso cómo esta arquitectura temprana, con su diseño sencillo, sentó las bases para las CNNs futuras. Su eficacia en la clasificación de dígitos a mano fue revolucionaria en su momento.  \n",
    "**AlexNet:** Lo que es destacable de AlexNet es cómo utilizó técnicas modernas y una arquitectura más profunda para lograr un rendimiento nunca antes visto en ImageNet, superando a otros enfoques tradicionales de procesamiento de imágenes. También es notable cómo popularizó las GPUs para entrenar redes neuronales, dado que fue entrenada usando GPUs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
